{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wine(dataset).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQ/v0PumH+wqceUlV6RCPD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukulsingh94868/ML-project/blob/supervisedML/wine(dataset).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3orWC9JMWN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing all the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2fcp7yJnmdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('wine.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyuwAqT1n1Rz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "c37e0ffe-bee0-4b0f-8f20-80f9c5fd66db"
      },
      "source": [
        "df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Wine</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic.acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Acl</th>\n",
              "      <th>Mg</th>\n",
              "      <th>Phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid.phenols</th>\n",
              "      <th>Proanth</th>\n",
              "      <th>Color.int</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD</th>\n",
              "      <th>Proline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>3</td>\n",
              "      <td>13.71</td>\n",
              "      <td>5.65</td>\n",
              "      <td>2.45</td>\n",
              "      <td>20.5</td>\n",
              "      <td>95</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.52</td>\n",
              "      <td>1.06</td>\n",
              "      <td>7.70</td>\n",
              "      <td>0.64</td>\n",
              "      <td>1.74</td>\n",
              "      <td>740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>3</td>\n",
              "      <td>13.40</td>\n",
              "      <td>3.91</td>\n",
              "      <td>2.48</td>\n",
              "      <td>23.0</td>\n",
              "      <td>102</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.41</td>\n",
              "      <td>7.30</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.56</td>\n",
              "      <td>750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>3</td>\n",
              "      <td>13.27</td>\n",
              "      <td>4.28</td>\n",
              "      <td>2.26</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.35</td>\n",
              "      <td>10.20</td>\n",
              "      <td>0.59</td>\n",
              "      <td>1.56</td>\n",
              "      <td>835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>3</td>\n",
              "      <td>13.17</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.37</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120</td>\n",
              "      <td>1.65</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.53</td>\n",
              "      <td>1.46</td>\n",
              "      <td>9.30</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.62</td>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>3</td>\n",
              "      <td>14.13</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2.74</td>\n",
              "      <td>24.5</td>\n",
              "      <td>96</td>\n",
              "      <td>2.05</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.35</td>\n",
              "      <td>9.20</td>\n",
              "      <td>0.61</td>\n",
              "      <td>1.60</td>\n",
              "      <td>560</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Wine  Alcohol  Malic.acid   Ash  ...  Color.int   Hue    OD  Proline\n",
              "0       1    14.23        1.71  2.43  ...       5.64  1.04  3.92     1065\n",
              "1       1    13.20        1.78  2.14  ...       4.38  1.05  3.40     1050\n",
              "2       1    13.16        2.36  2.67  ...       5.68  1.03  3.17     1185\n",
              "3       1    14.37        1.95  2.50  ...       7.80  0.86  3.45     1480\n",
              "4       1    13.24        2.59  2.87  ...       4.32  1.04  2.93      735\n",
              "..    ...      ...         ...   ...  ...        ...   ...   ...      ...\n",
              "173     3    13.71        5.65  2.45  ...       7.70  0.64  1.74      740\n",
              "174     3    13.40        3.91  2.48  ...       7.30  0.70  1.56      750\n",
              "175     3    13.27        4.28  2.26  ...      10.20  0.59  1.56      835\n",
              "176     3    13.17        2.59  2.37  ...       9.30  0.60  1.62      840\n",
              "177     3    14.13        4.10  2.74  ...       9.20  0.61  1.60      560\n",
              "\n",
              "[178 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE0SQSmepz_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = pd.get_dummies(df['Wine'])\n",
        "df = pd.concat([df,a],axis=1)\n",
        "X = df.drop([1, 2,3,'Wine'], axis = 1)\n",
        "y = df[[1,2,3]].values\n",
        "X_train, X_test, Y_train,Y_test = train_test_split(X, y, test_size=0.20,)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xulyK8W6qL58",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "28982c83-4e7c-40d4-b56a-51589ad4cd60"
      },
      "source": [
        "print(df.head())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Wine  Alcohol  Malic.acid   Ash   Acl   Mg  ...   Hue    OD  Proline  1  2  3\n",
            "0     1    14.23        1.71  2.43  15.6  127  ...  1.04  3.92     1065  1  0  0\n",
            "1     1    13.20        1.78  2.14  11.2  100  ...  1.05  3.40     1050  1  0  0\n",
            "2     1    13.16        2.36  2.67  18.6  101  ...  1.03  3.17     1185  1  0  0\n",
            "3     1    14.37        1.95  2.50  16.8  113  ...  0.86  3.45     1480  1  0  0\n",
            "4     1    13.24        2.59  2.87  21.0  118  ...  1.04  2.93      735  1  0  0\n",
            "\n",
            "[5 rows x 17 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiOHhuvLqf7V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "11c24112-2875-4723-eead-8c8bc284ddfc"
      },
      "source": [
        "print(df.tail())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Wine  Alcohol  Malic.acid   Ash   Acl   Mg  ...   Hue    OD  Proline  1  2  3\n",
            "173     3    13.71        5.65  2.45  20.5   95  ...  0.64  1.74      740  0  0  1\n",
            "174     3    13.40        3.91  2.48  23.0  102  ...  0.70  1.56      750  0  0  1\n",
            "175     3    13.27        4.28  2.26  20.0  120  ...  0.59  1.56      835  0  0  1\n",
            "176     3    13.17        2.59  2.37  20.0  120  ...  0.60  1.62      840  0  0  1\n",
            "177     3    14.13        4.10  2.74  24.5   96  ...  0.61  1.60      560  0  0  1\n",
            "\n",
            "[5 rows x 17 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srwsAbYfqikP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_prop(model,a0):\n",
        "    \n",
        "    # Load parameters from model\n",
        "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
        "    # Do the first Linear step \n",
        "    # Z1 is the input layer x times the dot product of the weights + bias b\n",
        "    z1 = a0.dot(W1) + b1\n",
        "    \n",
        "    # Put it through the first activation function\n",
        "    a1 = np.tanh(z1)\n",
        "    \n",
        "    # Second linear step\n",
        "    z2 = a1.dot(W2) + b2\n",
        "    \n",
        "    # Second activation function\n",
        "    a2 = np.tanh(z2)\n",
        "    \n",
        "    #Third linear step\n",
        "    z3 = a2.dot(W3) + b3\n",
        "    \n",
        "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
        "    a3 = softmax(z3)\n",
        "    \n",
        "    #Store all results in these values\n",
        "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
        "    return cache\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo785W6sqrHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(z):\n",
        "    #Calculate exponent term first\n",
        "    exp_scores = np.exp(z)\n",
        "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLrs0nGqqwp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_prop(model,cache,y):\n",
        "\n",
        "    # Load parameters from model\n",
        "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
        "    \n",
        "    # Load forward propagation results\n",
        "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
        "    \n",
        "    # Get number of samples\n",
        "    m = y.shape[0]\n",
        "    \n",
        "    # Calculate loss derivative with respect to output\n",
        "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
        "\n",
        "    # Calculate loss derivative with respect to second layer weights\n",
        "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
        "    \n",
        "    # Calculate loss derivative with respect to second layer bias\n",
        "    db3 = 1/m*np.sum(dz3, axis=0)\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer\n",
        "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer weights\n",
        "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer bias\n",
        "    db2 = 1/m*np.sum(dz2, axis=0)\n",
        "    \n",
        "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
        "    \n",
        "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
        "    \n",
        "    db1 = 1/m*np.sum(dz1,axis=0)\n",
        "    \n",
        "    # Store gradients\n",
        "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKN9Fc6Dq5uJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_loss(y,y_hat):\n",
        "    # Clipping value\n",
        "    minval = 0.000000000001\n",
        "    # Number of samples\n",
        "    m = y.shape[0]\n",
        "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
        "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugg0x1rJq-Ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_derivative(y,y_hat):\n",
        "    return (y_hat-y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc4kjGlRrB8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tanh_derivative(x):\n",
        "    return (1 - np.power(x, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCSHvIrUrGtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
        "    # First layer weights\n",
        "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
        "    \n",
        "    # First layer bias\n",
        "    b1 = np.zeros((1, nn_hdim))\n",
        "    \n",
        "    # Second layer weights\n",
        "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
        "    \n",
        "    # Second layer bias\n",
        "    b2 = np.zeros((1, nn_hdim))\n",
        "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
        "    b3 = np.zeros((1,nn_output_dim))\n",
        "    \n",
        "    \n",
        "    # Package and return model\n",
        "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBKuYQerrKg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(model,grads,learning_rate):\n",
        "    # Load parameters\n",
        "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
        "    \n",
        "    # Update parameters\n",
        "    W1 -= learning_rate * grads['dW1']\n",
        "    b1 -= learning_rate * grads['db1']\n",
        "    W2 -= learning_rate * grads['dW2']\n",
        "    b2 -= learning_rate * grads['db2']\n",
        "    W3 -= learning_rate * grads['dW3']\n",
        "    b3 -= learning_rate * grads['db3']\n",
        "    \n",
        "    # Store and return parameters\n",
        "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izbw2-sQrP15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, x):\n",
        "    # Do forward pass\n",
        "    c = forward_prop(model,x)\n",
        "    #get y_hat\n",
        "    y_hat = np.argmax(c['a3'], axis=1)\n",
        "    return y_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPq65W5XrUNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy,steps,losses = [],[],[]\n",
        "def train(model,X_,y_,learning_rate, iterations, print_loss=False):\n",
        "    # Gradient descent. Loop over epochs\n",
        "    for i in range(0, iterations):\n",
        "\n",
        "        # Forward propagation\n",
        "        cache = forward_prop(model,X_)\n",
        "\n",
        "        # Backpropagation\n",
        "        grads = backward_prop(model,cache,y_)\n",
        "        \n",
        "        # Gradient descent parameter update\n",
        "        # Assign new parameters to the model\n",
        "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
        "    \n",
        "        # Pring loss & accuracy every 100 iterations\n",
        "        if print_loss and i % 100 == 0:\n",
        "            a3 = cache['a3']\n",
        "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
        "            y_hat = predict(model,X_)\n",
        "            y_true = y_.argmax(axis=1)\n",
        "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
        "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZI15T5Hraz6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ac8a3a66-d2e7-40b5-c730-6efdf959bd61"
      },
      "source": [
        "model = initialize_parameters(nn_input_dim=13, nn_hdim= 5, nn_output_dim= 3)\n",
        "model = train(model,X_train,Y_train,learning_rate=0.07,iterations=4500,print_loss=True)\n",
        "plt.plot(losses)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after iteration 0 : 1.181512559344807\n",
            "Accuracy after iteration 0 : 50.0 %\n",
            "Loss after iteration 100 : 0.44630896044840057\n",
            "Accuracy after iteration 100 : 84.50704225352112 %\n",
            "Loss after iteration 200 : 0.36817649274526904\n",
            "Accuracy after iteration 200 : 87.32394366197182 %\n",
            "Loss after iteration 300 : 0.3312401086709115\n",
            "Accuracy after iteration 300 : 89.43661971830986 %\n",
            "Loss after iteration 400 : 0.2917894692625985\n",
            "Accuracy after iteration 400 : 91.54929577464789 %\n",
            "Loss after iteration 500 : 0.2646799028286522\n",
            "Accuracy after iteration 500 : 92.25352112676056 %\n",
            "Loss after iteration 600 : 0.23139148139506857\n",
            "Accuracy after iteration 600 : 92.95774647887323 %\n",
            "Loss after iteration 700 : 0.18599170326690176\n",
            "Accuracy after iteration 700 : 95.07042253521126 %\n",
            "Loss after iteration 800 : 0.15918991624021334\n",
            "Accuracy after iteration 800 : 95.77464788732394 %\n",
            "Loss after iteration 900 : 0.15064762943357987\n",
            "Accuracy after iteration 900 : 95.77464788732394 %\n",
            "Loss after iteration 1000 : 0.1452789648172091\n",
            "Accuracy after iteration 1000 : 95.77464788732394 %\n",
            "Loss after iteration 1100 : 0.1404007761540587\n",
            "Accuracy after iteration 1100 : 95.77464788732394 %\n",
            "Loss after iteration 1200 : 0.13372150021565743\n",
            "Accuracy after iteration 1200 : 95.77464788732394 %\n",
            "Loss after iteration 1300 : 0.1292934776308078\n",
            "Accuracy after iteration 1300 : 95.77464788732394 %\n",
            "Loss after iteration 1400 : 0.12328593111495328\n",
            "Accuracy after iteration 1400 : 95.77464788732394 %\n",
            "Loss after iteration 1500 : 0.10870135120628766\n",
            "Accuracy after iteration 1500 : 96.47887323943662 %\n",
            "Loss after iteration 1600 : 0.10566993666156284\n",
            "Accuracy after iteration 1600 : 96.47887323943662 %\n",
            "Loss after iteration 1700 : 0.10233059243050774\n",
            "Accuracy after iteration 1700 : 96.47887323943662 %\n",
            "Loss after iteration 1800 : 0.0983418497518847\n",
            "Accuracy after iteration 1800 : 96.47887323943662 %\n",
            "Loss after iteration 1900 : 0.0942277257097714\n",
            "Accuracy after iteration 1900 : 96.47887323943662 %\n",
            "Loss after iteration 2000 : 0.0891182471958748\n",
            "Accuracy after iteration 2000 : 96.47887323943662 %\n",
            "Loss after iteration 2100 : 0.08163855047369548\n",
            "Accuracy after iteration 2100 : 97.1830985915493 %\n",
            "Loss after iteration 2200 : 0.07485729863924244\n",
            "Accuracy after iteration 2200 : 97.88732394366197 %\n",
            "Loss after iteration 2300 : 0.06947967100793867\n",
            "Accuracy after iteration 2300 : 97.88732394366197 %\n",
            "Loss after iteration 2400 : 0.05555825627554747\n",
            "Accuracy after iteration 2400 : 98.59154929577466 %\n",
            "Loss after iteration 2500 : 0.05266008226286982\n",
            "Accuracy after iteration 2500 : 98.59154929577466 %\n",
            "Loss after iteration 2600 : 0.05087862040760104\n",
            "Accuracy after iteration 2600 : 98.59154929577466 %\n",
            "Loss after iteration 2700 : 0.0496257013813816\n",
            "Accuracy after iteration 2700 : 98.59154929577466 %\n",
            "Loss after iteration 2800 : 0.0486499977942401\n",
            "Accuracy after iteration 2800 : 98.59154929577466 %\n",
            "Loss after iteration 2900 : 0.047840946672831314\n",
            "Accuracy after iteration 2900 : 98.59154929577466 %\n",
            "Loss after iteration 3000 : 0.04714321300451003\n",
            "Accuracy after iteration 3000 : 98.59154929577466 %\n",
            "Loss after iteration 3100 : 0.046525450146707496\n",
            "Accuracy after iteration 3100 : 98.59154929577466 %\n",
            "Loss after iteration 3200 : 0.04596821173816299\n",
            "Accuracy after iteration 3200 : 98.59154929577466 %\n",
            "Loss after iteration 3300 : 0.045458608050542484\n",
            "Accuracy after iteration 3300 : 98.59154929577466 %\n",
            "Loss after iteration 3400 : 0.04498763858752871\n",
            "Accuracy after iteration 3400 : 98.59154929577466 %\n",
            "Loss after iteration 3500 : 0.04454872351485894\n",
            "Accuracy after iteration 3500 : 98.59154929577466 %\n",
            "Loss after iteration 3600 : 0.04413683272251335\n",
            "Accuracy after iteration 3600 : 98.59154929577466 %\n",
            "Loss after iteration 3700 : 0.04374793981552279\n",
            "Accuracy after iteration 3700 : 98.59154929577466 %\n",
            "Loss after iteration 3800 : 0.04337866008365851\n",
            "Accuracy after iteration 3800 : 98.59154929577466 %\n",
            "Loss after iteration 3900 : 0.04302598803103514\n",
            "Accuracy after iteration 3900 : 98.59154929577466 %\n",
            "Loss after iteration 4000 : 0.042687075072165466\n",
            "Accuracy after iteration 4000 : 98.59154929577466 %\n",
            "Loss after iteration 4100 : 0.04235899335750512\n",
            "Accuracy after iteration 4100 : 98.59154929577466 %\n",
            "Loss after iteration 4200 : 0.04203841507951423\n",
            "Accuracy after iteration 4200 : 98.59154929577466 %\n",
            "Loss after iteration 4300 : 0.0417210903146501\n",
            "Accuracy after iteration 4300 : 98.59154929577466 %\n",
            "Loss after iteration 4400 : 0.041400956486409\n",
            "Accuracy after iteration 4400 : 98.59154929577466 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7a544b9320>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAVK4i4gESOEuIhIghbuISIAU7iIiAfp/k/0xl2K4qe0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K2F6jvfrgDD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41f40933-7da4-494f-cc27-9d5bec65e5ac"
      },
      "source": [
        "test = predict(model,X_test)\n",
        "test = pd.get_dummies(test)\n",
        "Y_test = pd.DataFrame(Y_test)\n",
        "print(\"Testing accuracy is: \",str(accuracy_score(Y_test, test) * 100)+\"%\")\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing accuracy is:  86.11111111111111%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ene3wEYTrlQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}